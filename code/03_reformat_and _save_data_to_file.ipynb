{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d310f82-e73c-42a1-847d-52e82f81ab37",
   "metadata": {},
   "source": [
    "# Reformat and save data to file\n",
    "\n",
    "Great, now we know how to access the data we need. Our next step will be to structure it in a way that will be useful to us. \n",
    "\n",
    "Let's think about what the important bits of information we need from the full data set. \n",
    "\n",
    "About the neurons, we need to know:\n",
    "1. Spike times for each recorded and clustered neuron (remember we call them 'units')\n",
    "2. Which brain area the neuron belongs to: is it CA1, or Prefrontal Cortex (PFC)\n",
    "3. What task was the animal doing?\n",
    "4. What were the x and y positions of the animal?\n",
    "\n",
    "Unfortunately for us, all this information is in different files. All of them are nested, so organizing this may not be straightforward. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "471670ff-8808-4bb5-a302-16cefae56537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as spio # you will need this library to import .mat data into python\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "# we'll use glob and pathlib.Path to get the names all files\n",
    "# that end with the extension .mat in the folder (also referred to as 'directory') \n",
    "# that we're interested in \n",
    "import glob\n",
    "\n",
    "from pathlib import Path # this allows us to only import \n",
    "                         # the function Path from the library \n",
    "                         # pathlib rather than the full library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80e88c7-25c5-42fe-a553-91fd98c7ce44",
   "metadata": {},
   "source": [
    "We can always type out the file names and import them one by one manually, but the method shown here will be easier to generalize. That is, once you finish setting up this process, you can easily reuse it for a different animal with making only minor changes, rathere than having to type out all the file names again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dd93544-17b9-4ea4-91ca-c93478e341a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '../data/N2/' # here, you specify which folder you want to look at\n",
    "                          # if you're later interested in repeating this process for \n",
    "                          # another animal, simply change the name of the folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10e287bb-62a4-446f-a334-16a6596e5c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# glob is a function that automaticall gets the file paths (or addresses) of all files \n",
    "# that end with the .mat extension. The * simply means that the name could be anything, \n",
    "# and the only requirement is that the file extension is .mat. If you're loooking for images, \n",
    "# you can specify *.jpg, or *.bmp, etc. \n",
    "\n",
    "# as an aside, it's impossible to memorize that these functions exist. There are many others that do \n",
    "# similar things. The only reason I know is because I needed this functionality and I googled: \n",
    "# 'find file names in folder python' or something along those lines. And it eventually led me to these\n",
    "# functions. Google is your friend.\n",
    "\n",
    "file_paths = glob.glob(directory + '*.mat') # remember you can put strings together using the + operator\n",
    "                                            # so directory + '*.mat' = '../data/N2/*.mat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ea3ac20-6fd4-4d40-9b43-f4159b6a0589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/N2\\\\N2cellinfo.mat',\n",
       " '../data/N2\\\\N2eeg.mat',\n",
       " '../data/N2\\\\N2rawpos.mat',\n",
       " '../data/N2\\\\N2spikes.mat',\n",
       " '../data/N2\\\\N2task.mat',\n",
       " '../data/N2\\\\N2tetinfo.mat']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_paths # we now have addresses to every file in the folder that ends with .mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9e7bf7-f782-4eae-9761-be75a5fac39d",
   "metadata": {},
   "source": [
    "Now we can go through our list and get data from each file as needed. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b785f5-4f4b-47b0-9579-937a144a1c4e",
   "metadata": {},
   "source": [
    "### **Step 1:**   \n",
    "The `tetinfo` file will have information about which brain are the tetrode was in that particular day. So we can easily label any neuron recorded from that tetrode as belonging to the corresponding brain area. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14b5f09c-a800-4b8f-9549-636b6806c107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/N2\\\\N2tetinfo.mat'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look for the tetinfo.mat file. \n",
    "# we'll loop through the list of file names\n",
    "# then we'll see if any of them have the word 'tetinfo' in them. \n",
    "# if the string method .find() finds the word 'tetinfo' it will return \n",
    "# the index of the position where it found the word. If it doesn't, it will return -1. \n",
    "# So we can say, that if the output of .find() is not -1, then that's the name we want.\n",
    "\n",
    "\n",
    "for name in file_paths:\n",
    "    if name.find('tetinfo') != -1:\n",
    "        matfile = name\n",
    "\n",
    "matfile # now we have the address to tetinfo.mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adee8fa9-ce57-4b62-b226-bf842a04b87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will load this file same as before and use the dictionary key to access the data in it. \n",
    "tetinfo = spio.loadmat(matfile, squeeze_me=True)\n",
    "tetinfo = tetinfo['tetinfo']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ab465a-26f1-426a-acb2-f129090880df",
   "metadata": {},
   "source": [
    "My goal here is to convert the whole nested array into a dictionary so we can easily access all elements. \n",
    "Dictionaries have `keys` and `values`. There are different ways to define or crreate a dictionary. \n",
    "\n",
    "Method 1: manually defining each key-value pair\n",
    "```\n",
    "dictionary_1 = {\n",
    "    'key_1' : [1, 2, 3],\n",
    "    'key_2' : ['a', 'b', 'c']\n",
    "}\n",
    "```\n",
    "\n",
    "Method 2: using two lists and the `zip()` function to automatically match them.\n",
    "```\n",
    "keys = ['key_1', 'key_2']                # we define a list of keys\n",
    "values = [[1, 2, 3], ['a', 'b', 'c']]    # we define a list of values\n",
    "dictionary_2 = dict(zip(keys, values))   # we use the zip() function to pair them up by element. \n",
    "```\n",
    "\n",
    "in Method 2, we use `zip()` to match two lists of the same length. `zip()` will automatically match `key_1` with the first list `[1, 2, 3]`, and `key_2` with the second list `['a', 'b', 'c']`. Both methods will result in the same output. the second one just more convenient to use in our case here. You can copy and paste both pieces of code and compare `dictionary_1` to `dictionary_2`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6998817-363c-4807-9082-a5ec692429b3",
   "metadata": {},
   "source": [
    "**Taking apart and storing the first level of data: days**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55798fd8-0294-41b0-ae50-002f281427af",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_days = tetinfo.size # number of days = the number of elements in the first level\n",
    "\n",
    "# next, we create lists of keys and values\n",
    "day_keys = ['day' + str(num) for num in range(num_days)] # this is a list comprehension. \n",
    "                                                         # It's a shortened form of a for-loop\n",
    "                                                         # It says: day_keys is a list (indicated by the []),\n",
    "                                                         # each element of the list is the string 'day' plus numbers \n",
    "                                                         # in the range of 0 to the number of days. \n",
    "                    \n",
    "day_values = [day for day in tetinfo] # another list comprehension that says day_values is a list and \n",
    "                                       # each element is one element in the array tetinfo\n",
    "\n",
    "N2_tetinfo = dict(zip(day_keys, day_values)) # here we use that zip() function to pair these two lists \n",
    "                                             # and make them into a dictionary\n",
    "\n",
    "del([day_keys, day_values]) # delete unnecessary variables from memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9adc1645-a0c9-4e12-b754-823f566e6366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N2_tetinfo is of type : <class 'dict'>\n",
      "with keys: dict_keys(['day0', 'day1', 'day2', 'day3', 'day4', 'day5', 'day6', 'day7', 'day8', 'day9', 'day10', 'day11'])\n"
     ]
    }
   ],
   "source": [
    "# now, if we look at what is in N2_tetinfo\n",
    "print(f'N2_tetinfo is of type : {type(N2_tetinfo)}')\n",
    "print(f'with keys: {N2_tetinfo.keys()}')\n",
    "\n",
    "# we see it is a dictionary with each day as a key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94cecbd1-441f-436a-b9f4-4276488a4ea7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([], dtype=float64),\n",
       "       array([array([], dtype=float64),\n",
       "              array(('PFC', 72), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('PFC', 59), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('PFC', 59), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('PFC', 78), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('PFC', 66), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('PFC', 60), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('PFC', 73), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('PFC', 61), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('PFC', 39), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('PFC', 58), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('PFC', 48), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('PFC', 48), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('PFC', 38), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array([], dtype=float64),\n",
       "              array(('CA1', 94), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('CA1', 90), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('CA1', 81), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('CA1', 92), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('CA1', 90), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('CA1', 91), dtype=[('area', 'O'), ('depth', 'O')])],\n",
       "             dtype=object)                                               ,\n",
       "       array([], dtype=float64),\n",
       "       array([array([], dtype=float64),\n",
       "              array(('PFC', 72), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('PFC', 59), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('PFC', 59), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('PFC', 78), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('PFC', 66), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('PFC', 60), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('PFC', 73), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('PFC', 61), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('PFC', 39), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('PFC', 58), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('PFC', 48), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('PFC', 48), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('PFC', 38), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array([], dtype=float64),\n",
       "              array(('CA1', 94), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('CA1', 90), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('CA1', 81), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('CA1', 92), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('CA1', 90), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('CA1', 91), dtype=[('area', 'O'), ('depth', 'O')])],\n",
       "             dtype=object)                                               ,\n",
       "       array([], dtype=float64),\n",
       "       array([array([], dtype=float64),\n",
       "              array(('PFC', 72), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('PFC', 59), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('PFC', 59), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('PFC', 78), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('PFC', 66), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('PFC', 60), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('PFC', 73), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('PFC', 61), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('PFC', 39), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('PFC', 58), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('PFC', 48), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('PFC', 48), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('PFC', 38), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array([], dtype=float64),\n",
       "              array(('CA1', 94), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('CA1', 90), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('CA1', 81), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('CA1', 92), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('CA1', 90), dtype=[('area', 'O'), ('depth', 'O')]),\n",
       "              array(('CA1', 91), dtype=[('area', 'O'), ('depth', 'O')])],\n",
       "             dtype=object)                                               ],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can access each day's data using the key for that day. \n",
    "N2_tetinfo['day0']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f0665f-0d3a-49e4-8fed-1cc5a9f4cb8b",
   "metadata": {},
   "source": [
    "**Taking apart and storing the second level of data: epochs**  \n",
    "The next step will be to make each day into a dictionary where the keys are the epochs and we can access data belonging to each epoch using the appropriate key. Because we're looking at locations of tetrodes here, we do not need to go any deeper into this file. We can pick any epoch of one day and use the location of the tetrode in that epoch for all epochs in that day. So if the tetrode was in PFC for epoch0 of day1, we can assume that it was in PFC for the rest of the epochs as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afbf6173-7d3f-4b13-bfd3-c2d15c049037",
   "metadata": {},
   "outputs": [],
   "source": [
    "for day_key in N2_tetinfo.keys():                        # we first cycle through each day\n",
    "    day_data = N2_tetinfo[day_key]                       # we put all the data for that day in a variable called day_data\n",
    "        \n",
    "    for epoch in day_data:                               # then we cycle through each epoch in the day and find one that isn't empty.\n",
    "        if epoch.size > 0:                                               # I only thought to check for this because I noticed not all epochs had data in them\n",
    "            tt_info = epoch                              # We can then use the first epoch we find that has data in it and \n",
    "            break                                                        # break out of the loop without checking any others \n",
    "    tt_name = []                                         # We'll create two empty lists to use as the keys and values\n",
    "    tt_loc = []\n",
    "    for tt in range(tt_info.size):                       # cycle through each tetrode in that epoch\n",
    "        if tt_info[tt].size > 0:                         # some tetrodes have no data about location. They were probably used as ground. We can skip those \n",
    "            tt_name.append('tt'+ str(tt))                # then append the tetrode name and tetrode location to the appropriate lists\n",
    "            tt_loc.append(tt_info[tt].item()[0])\n",
    "    N2_tetinfo[day_key] = dict(zip(tt_name, tt_loc))     # create a dictionary from the list and place it in the value place for that day. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dfd5fc1-940f-4dc0-87c1-2073caba9db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tt1': 'PFC',\n",
       " 'tt2': 'PFC',\n",
       " 'tt3': 'PFC',\n",
       " 'tt4': 'PFC',\n",
       " 'tt5': 'PFC',\n",
       " 'tt6': 'PFC',\n",
       " 'tt7': 'PFC',\n",
       " 'tt8': 'PFC',\n",
       " 'tt9': 'PFC',\n",
       " 'tt10': 'PFC',\n",
       " 'tt11': 'PFC',\n",
       " 'tt12': 'PFC',\n",
       " 'tt13': 'PFC',\n",
       " 'tt15': 'CA1',\n",
       " 'tt16': 'CA1',\n",
       " 'tt17': 'CA1',\n",
       " 'tt18': 'CA1',\n",
       " 'tt19': 'CA1',\n",
       " 'tt20': 'CA1'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so now, if we want to see the locations of the tetrodes for day0:\n",
    "\n",
    "N2_tetinfo['day0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd85004e-5bcd-4654-8fd7-c84d24cd7dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CA1'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or if we want to specifically know where tetrode 16 was on day3:\n",
    "N2_tetinfo['day3']['tt16']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bed301-3da2-49a8-a3e0-125ba167e471",
   "metadata": {},
   "source": [
    "### **Step 2:**   \n",
    "The `task` file will have information about what the animal was doing during that epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4620e082-9478-4287-95a5-de1cc28b5f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/N2\\\\N2task.mat'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look for the task.mat file. \n",
    "# we'll loop through the list of file names\n",
    "# then we'll see if any of them have the word 'task' in them. \n",
    "# if the string method .find() finds the word 'task' it will return \n",
    "# the index of the position where it found the word. If it doesn't, it will return -1. \n",
    "# So we can say, that if the output of .find() is not -1, then that's the name we want.\n",
    "\n",
    "\n",
    "for name in file_paths:\n",
    "    if name.find('task') != -1:\n",
    "        matfile = name\n",
    "\n",
    "matfile # now we have the address to tetinfo.mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8ce26c3-3a57-4438-b670-49442df66ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will load this file same as before and use the dictionary key to access the data in it. \n",
    "task = spio.loadmat(matfile, squeeze_me=True)\n",
    "task = task['task']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc97ffc-63ba-49fc-b02e-13025ecf0b70",
   "metadata": {},
   "source": [
    "**Taking apart and storing the first level of data: days**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21744679-cb50-48f1-b043-b1acf9856f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_days = task.size # number of days = the number of elements in the first level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1052af47-45c0-4f1c-a183-1b3a2eec0dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next, we create lists of keys and values\n",
    "day_keys = ['day' + str(num) for num in range(num_days)] # this is a list comprehension. \n",
    "                                                         # It's a shortened form of a for-loop\n",
    "                                                         # It says: day_keys is a list (indicated by the []),\n",
    "                                                         # each element of the list is the string 'day' plus numbers \n",
    "                                                         # in the range of 0 to the number of days. \n",
    "                    \n",
    "day_values = [day for day in task] # another list comprehension that says day_values is a list and \n",
    "                                       # each element is one element in the array tetinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ca01d5f-fecb-4427-bf9a-14f3d3b02dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "N2_task = dict(zip(day_keys, day_values)) # here we use that zip() function to pair these two lists \n",
    "                                             # and make them into a dictionary\n",
    "\n",
    "del([day_keys, day_values]) # delete unnecessary variables from memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e13dc705-97d8-4d73-ab99-aed8083abb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N2_task is of type : <class 'dict'>\n",
      "with keys: dict_keys(['day0', 'day1', 'day2', 'day3', 'day4', 'day5', 'day6', 'day7', 'day8', 'day9'])\n"
     ]
    }
   ],
   "source": [
    "# now, if we look at what is in N2_tetinfo\n",
    "print(f'N2_task is of type : {type(N2_task)}')\n",
    "print(f'with keys: {N2_task.keys()}')\n",
    "\n",
    "# we see it is a dictionary with each day as a key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a49d193-2582-478c-b8a0-c05df7bf5f16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([], dtype=float64), array(('Run',), dtype=[('type', 'O')]),\n",
       "       array([], dtype=float64), array(('Run',), dtype=[('type', 'O')]),\n",
       "       array([], dtype=float64), array(('Run',), dtype=[('type', 'O')])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can access each day's data using the key for that day. \n",
    "N2_task['day5']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae02485-abe8-4122-8afa-a720204fa005",
   "metadata": {},
   "source": [
    "**Taking apart and storing the second level of data: epochs**  \n",
    "The next step will be to make each day into a dictionary where the keys are the epochs and we can access data belonging to each epoch using the appropriate key. Because we're looking at locations of tetrodes here, we do not need to go any deeper into this file. We can pick any epoch of one day and use the location of the tetrode in that epoch for all epochs in that day. So if the tetrode was in PFC for epoch0 of day1, we can assume that it was in PFC for the rest of the epochs as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85229c61-0605-4c6f-b832-a5dad1cffe7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14da4b10-3c06-4d84-af99-dc4cdb6838d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for day_key in N2_task.keys():\n",
    "    # print(day_key)\n",
    "    day_data = N2_task[day_key]\n",
    "    num_epochs = day_data.size\n",
    "    # print(num_epochs)\n",
    "    epoch_keys = []\n",
    "    epoch_values = []\n",
    "    for num, epoch in enumerate(day_data):\n",
    "        # print(num, epoch.size)\n",
    "        if epoch.size > 0:                        # I only thought to check for this because I noticed not all epochs had data in them\n",
    "            task = epoch[()][0]                                   \n",
    "            epoch_key = 'epoch' + str(num)\n",
    "            epoch_keys.append(epoch_key)\n",
    "            epoch_values.append(task)\n",
    "    N2_task[day_key] = dict(zip(epoch_keys, epoch_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a094d95e-31b3-47f6-b1b9-aebde4000cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Run'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N2_task['day0']['epoch1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99171b5-e2a8-4523-a2d0-1cffb9cbfea4",
   "metadata": {},
   "source": [
    "### **Step 2:**  \n",
    "Now that we know where each tetrode was on each day, we can safely assume that any neuron recorded from that tetrode was from that location. So if the tetrode was in CA1, any neuron from that tetrode is from CA1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62691cc-9332-41ea-9702-e663b039f14b",
   "metadata": {},
   "source": [
    "As before let's extract the filename of interest, get its address and load the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ced6048-cd20-4b1e-b402-a32783c319f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/N2\\\\N2spikes.mat'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for name in file_paths:\n",
    "    if name.find('spikes') != -1:\n",
    "        matfile = name\n",
    "\n",
    "matfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "630020b6-631d-4bef-8791-9bc8e29fff82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll do the same thing as before and use spio to load the .mat file\n",
    "\n",
    "data = spio.loadmat(matfile, squeeze_me=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11883962-2f1a-4aaf-9157-e8b8574fbb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data['spikes']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b451bb-5eb5-4d4e-a272-d133a0ab3893",
   "metadata": {},
   "source": [
    "**Taking apart and storing the first level of data: days**  \n",
    "Here, just like before we take change the organization of data so that `N2_data` is a dictionary with the day number as the key and the data from that day as the value paired with that key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "921eb374-cc94-4a9d-856b-894716680b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_days = data.size\n",
    "day_keys = ['day' + str(num) for num in range(num_days)]\n",
    "day_values = [day for day in data]\n",
    "N2_data = dict(zip(day_keys, day_values))\n",
    "del([day_keys, day_values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "517403ff-4ccd-4e5d-8561-2885d8ea29a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['day0', 'day1', 'day2', 'day3', 'day4', 'day5', 'day6', 'day7', 'day8', 'day9'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N2_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135b9781-2d11-4942-9bdb-6bb2aa23e3a1",
   "metadata": {},
   "source": [
    "**Taking apart and storing the second level of data: epochs**  \n",
    "Next, we will change the organization of data so that each day in `N2_data` is a dictionary with the epoch number as the key and the data from that epoch as the value paired with that key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3dc61d16-27ad-4d72-8c0b-156a6a17c5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in N2_data.keys():\n",
    "    day_data = N2_data[key]\n",
    "    num_epochs = day_data.size\n",
    "    epoch_keys = [key + '_epoch' +str(num) for num in range(num_epochs)]\n",
    "    epoch_values = [epoch for epoch in day_data]\n",
    "    N2_data[key] = dict(zip(epoch_keys, epoch_values))\n",
    "    del([epoch_keys, epoch_values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d4ef9165-8c87-42e6-a6e2-49538fcd31da",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = f'../clean_data/N2/'                                           # we want to eventually save this data to a folder on our computer, \n",
    "                                                                                # so this defines where that will be\n",
    "if not os.path.exists(save_path):                                          # here we first check to make sure that folder doesn't already exist\n",
    "            os.makedirs(save_path)                                         # if it doesn't exist, we create it. If it does, we don't bother.\n",
    "\n",
    "        \n",
    "# now we cycle through the data:\n",
    "\n",
    "for day in N2_data.keys():                                                 # cycle through each day\n",
    "    for epoch in N2_data[day].keys():                                      # cycle through each epoch of each day\n",
    "        num_tetrodes = N2_data[day][epoch].size                            # the number of tetrodes is just the size of that epoch array\n",
    "        cell_keys = []                                                     # create empty lists to store unit (neuron or cell) data, we make one for keys\n",
    "        cell_values = []                                                   # and one for values\n",
    "        \n",
    "        for tt in range(num_tetrodes):                                     # cycle through each tetrode one by one\n",
    "            num_cells = N2_data[day][epoch][tt].size                       # number of cells recorded on that tetrode = the size of the array\n",
    "            for cell in range(num_cells):                                  # cycle through each cell to extract spike data for that cell\n",
    "                cell_name = 'tt' + str(tt) + '_unit' + str(cell)           # create a name for the cell which includes which tetrode it's from\n",
    "                if (num_cells == 1):                                       # if there's just one cell on that tetrode we have to use .item()[0]\n",
    "                    spike_data = N2_data[day][epoch][tt].item()[0]\n",
    "                elif num_cells > 1:\n",
    "                    if N2_data[day][epoch][tt][cell].size > 0:             # I had to add this size condition in because there were some cells with no spike data in them at all\n",
    "                        spike_data = N2_data[day][epoch][tt][cell].item()[0]   # otherwise, we index using the cell variable and then use .item()[0]\n",
    "                \n",
    "                try:                                    # some tetrodes do not have corresponding location info so trying to pull it out of the ...\n",
    "                    cell_loc = N2_tetinfo[day][f'tt{tt}']      # ... tetinfo dictionary results in a 'key error'. The 'try', 'except' lines allow you to... \n",
    "                except:                                        # ... tell python what to do if an error pops up. In this case I'm saying,... \n",
    "                    cell_loc = []                              # ... if you encounter an error, just create an empty variable.\n",
    "                \n",
    "                if spike_data.ndim == 2:                       # I noticed that some cells have no extracted spikes on that so... \n",
    "                    cell_data = {                                  # ...we can skip these. For the ones that are 2-dimensional...\n",
    "                        'cell_location': cell_loc,                 # ...we get each column from spike_data and save it in the appropriate...\n",
    "                        'spike_times' : list(spike_data[:,0]),     # ... variable\n",
    "                        'xpos' : list(spike_data[:,1]),\n",
    "                        'ypos' : list(spike_data[:,2])\n",
    "                    }\n",
    "                    cell_keys.append(cell_name)                          # becauase we want to make this a dictionary, we append cell_name to cell_keys\n",
    "                    cell_values.append(cell_data)                        # and cell_data to cell_values\n",
    "        \n",
    "       \n",
    "        # now we have to save the data into a file on our computer\n",
    "                                                                               \n",
    "        df = pd.DataFrame(dict(zip(cell_keys, cell_values))).T           # using pandas, we create a dataframe from a dictionary, more on that below.\n",
    "        if df.size > 0:                                                  # we then check to see if there is actually any data in it, if there isn't don't save it\n",
    "            df.to_csv(f'{save_path}/{epoch}.csv', index=False)           # if it does, save it. \n",
    "        \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c97891-5e20-4c59-8309-8438cf1eddac",
   "metadata": {},
   "source": [
    "**Just a little about dataframes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eb6071-cb19-4cd0-8e27-0cb0a97745b2",
   "metadata": {},
   "source": [
    "Dataframes are like excel sheets. They have rows and columns and the columns have `column names`, and the rows have `indexes`\n",
    "You can create a dataframe in multiple ways. I'm going to demonstrate the way we used above in our code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9278a42e-83b6-492d-9904-007d8882402f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first I make a dictionary\n",
    "\n",
    "dictionary = {\n",
    "    'items' : ['apple', 'banana', 'sky', 'leaves'],\n",
    "    'color' : ['red', 'yellow', 'blue', 'green']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "33037297-6e02-4a0d-ad61-b9f1d2e1d483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'items': ['apple', 'banana', 'sky', 'leaves'],\n",
       " 'color': ['red', 'yellow', 'blue', 'green']}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the dictionary\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "92d0676a-9de5-4d9f-894e-5f85b82452e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>items</th>\n",
       "      <th>color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apple</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>banana</td>\n",
       "      <td>yellow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sky</td>\n",
       "      <td>blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>leaves</td>\n",
       "      <td>green</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    items   color\n",
       "0   apple     red\n",
       "1  banana  yellow\n",
       "2     sky    blue\n",
       "3  leaves   green"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to turn this dictionary into a dataframe, I just say:\n",
    "pd.DataFrame(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a644d8a7-12ab-49d6-9276-e4e26c8d46b3",
   "metadata": {},
   "source": [
    "if you noticed, I had a `.T` at the end of the line that defined the dataframe in the big block of code that saves our data to file. This just transposes the dataframe. That is, it switches the rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c442524e-0e5c-4556-b489-52120e65dd94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>items</th>\n",
       "      <td>apple</td>\n",
       "      <td>banana</td>\n",
       "      <td>sky</td>\n",
       "      <td>leaves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color</th>\n",
       "      <td>red</td>\n",
       "      <td>yellow</td>\n",
       "      <td>blue</td>\n",
       "      <td>green</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0       1     2       3\n",
       "items  apple  banana   sky  leaves\n",
       "color    red  yellow  blue   green"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dictionary).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03140ad-acc4-4b5e-bf2e-bd6123ae5d79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
